# Neural Storyteller â€“ Image Captioning with CNN-LSTM Seq2Seq Architecture

A deep learning project that generates natural language descriptions for images using a Sequence-to-Sequence (Seq2Seq) architecture with PyTorch.

## ğŸ¯ Project Overview

This project implements an end-to-end image captioning system that:
- Extracts visual features from images using pretrained ResNet50 (CNN)
- Generates natural language captions using an LSTM-based Seq2Seq decoder
- Supports both Greedy Search (fast) and Beam Search (high-quality) inference
- Provides comprehensive evaluation metrics (BLEU-4, Precision, Recall, F1)
- Includes a production-ready Streamlit web application with modern UI

## ğŸ“ Project Structure

```
GenAss/
â”œâ”€â”€ app.py                              # Streamlit web application
â”œâ”€â”€ GenAi-01_neural_storyteller_notebook.ipynb  # Training notebook (Kaggle)
â”œâ”€â”€ README.md                           # Project documentation
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ image_captioning_model.pth         # Trained model weights
â”œâ”€â”€ vocab.pkl                          # Vocabulary dictionary
â”œâ”€â”€ inv_vocab.pkl                      # Inverse vocabulary mapping
â””â”€â”€ flickr30k_features.pkl             # Cached ResNet50 features
```

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/muneeb-codehub/Gen-Ai-Assignment-Neural-Storyteller-Image-Captioning-using-CNN-LSTM-Seq2Seq-Architecture.git
cd Gen-Ai-Assignment-Neural-Storyteller-Image-Captioning-using-CNN-LSTM-Seq2Seq-Architecture
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

Or install manually:
```bash
pip install torch torchvision streamlit pillow pandas numpy matplotlib scikit-learn nltk tqdm
```

### 3. Download Model Files

Download the pre-trained model files and place them in the project root:
- `image_captioning_model.pth` - Trained LSTM decoder weights
- `vocab.pkl` - Vocabulary mapping
- `inv_vocab.pkl` - Inverse vocabulary
- `flickr30k_features.pkl` - Cached ResNet50 features (optional, only for training)

### 4. Run the Streamlit App

```bash
streamlit run app.py
```

The web app will open at `http://localhost:8501`

## ğŸ“ Training from Scratch (Kaggle)

### Prerequisites
1. Create a Kaggle account
2. Enable GPU acceleration (T4 x2 recommended)
3. Add dataset: `adityajn105/flickr30k`

### Steps
1. Upload `GenAi-01_neural_storyteller_notebook.ipynb` to Kaggle
2. Enable GPU T4 x2 accelerator in notebook settings
3. Run all cells sequentially
4. Download generated files:
   - `image_captioning_model.pth`
   - `vocab.pkl`
   - `inv_vocab.pkl`
   - `flickr30k_features.pkl`

The training takes approximately 2-3 hours on dual T4 GPUs.

## ğŸ—ï¸ Architecture

### Encoder
- Input: 2048-dim ResNet50 features
- Output: 512-dim hidden state
- Architecture: Single linear layer

### Decoder
- Embedding: vocab_size â†’ 512
- LSTM: 512 hidden units
- Output: Linear layer â†’ vocab_size
- Uses teacher forcing during training

### Why LSTM over GRU?
LSTM has separate cell state and hidden state, providing better long-term memory for longer captions compared to GRU.

## ğŸ“Š Evaluation Metrics

### BLEU-4 Score
Measures n-gram overlap between generated and reference captions.

### Token-level Metrics
- **Precision**: Accuracy of predicted tokens
- **Recall**: Coverage of ground truth tokens
- **F1-Score**: Harmonic mean of precision and recall

### Important Note on Metrics
Image captioning is subjective. Multiple correct captions can describe the same image with different wording:
- Ground Truth: "a man riding a bike"
- Generated: "a person cycling on the road"

Both are semantically correct, but metrics penalize lexical variations. **Semantic correctness is more meaningful than exact lexical matching.**

## ğŸ¨ Streamlit App Features

- **Gradient UI** with glassmorphism effects
- **Dual inference methods**: Greedy Search (faster) and Beam Search (better quality)
- **Real-time caption generation**
- **Responsive design** with smooth animations
- **Professional typography** using Google Fonts

## ğŸ“ Deliverables

1. âœ… **Caption Examples**: 5 random test images with ground truth and generated captions
2. âœ… **Loss Curve**: Training and validation loss visualization
3. âœ… **Quantitative Evaluation**: BLEU-4, Precision, Recall, F1-score
4. âœ… **App Deployment**: Streamlit app with beautiful UI

## ğŸ”§ Technical Details

- **Platform**: Kaggle Notebook
- **Accelerator**: GPU T4 x2 (Dual GPU)
- **Dataset**: Flickr30k (31,000+ images)
- **Framework**: PyTorch
- **Epochs**: 15
- **Optimizer**: Adam (lr=1e-3)
- **Loss Function**: CrossEntropyLoss (ignore padding)
- **Batch Size**: 64

## ğŸ“š Key Concepts

### Feature Caching
Instead of training CNN alongside RNN (computationally expensive), we:
1. Extract features once using pretrained ResNet50
2. Cache features to disk
3. Use cached features during caption training

This approach is:
- Much faster
- Requires less GPU memory
- Industry standard practice

### Inference Methods

**Greedy Search**
- Selects highest probability word at each step
- Fast but may miss better overall sequences
- O(n) complexity

**Beam Search**
- Maintains top-k candidates at each step
- Better quality captions
- O(kÃ—n) complexity

## ğŸ“ Academic Context

This project demonstrates:
- Multimodal deep learning (vision + language)
- Sequence-to-sequence architectures
- Transfer learning with pretrained CNNs
- Proper evaluation of generative models
- Production deployment of ML models

## ğŸ“– References

- ResNet: "Deep Residual Learning for Image Recognition" (He et al., 2015)
- Seq2Seq: "Sequence to Sequence Learning with Neural Networks" (Sutskever et al., 2014)
- Image Captioning: "Show and Tell: A Neural Image Caption Generator" (Vinyals et al., 2015)
- Dataset: Flickr30k (31,783 images with 158,915 captions)

## ğŸ“§ Contact

**Muneeb Arif**  
ğŸ“§ Email: muneebarif226@gmail.com  
ğŸ”— GitHub: [@muneeb-codehub](https://github.com/muneeb-codehub)  


*Generative AI Assignment - Image Captioning with CNN-LSTM Seq2Seq Architecture*
